{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Convolutional Neural Network (CNN) from Scratch using NumPy\n",
    "\n",
    "This notebook demonstrates a complete implementation of a Convolutional Neural Network (CNN) from scratch using only NumPy - without relying on high-level libraries like TensorFlow or PyTorch.\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'></u><font color='purple'>  : <b>Convolution operation</b><br> with a filter of 3x3 and a stride of 1 (stride = amount we move the window each time we slide) </center></caption>\n",
    "    \n",
    "\n",
    "### What’s Implemented:\n",
    "- **Forward Pass**\n",
    "  - Convolutional layers\n",
    "  - ReLU activation\n",
    "  - Max Pooling\n",
    "  - Flattening\n",
    "  - Fully Connected (Dense) layer\n",
    "  - Softmax output\n",
    "\n",
    "- **Backward Pass**\n",
    "  - Gradients for all layers using chain rule\n",
    "  - Backpropagation through Conv, Pool, and Dense layers\n",
    "\n",
    "- **Training Loop**\n",
    "  - Forward + backward pass per epoch\n",
    "  - Cross-entropy loss computation\n",
    "  - Gradient Descent parameter updates\n",
    "  - Training loss visualization\n",
    "\n",
    "- **Testing**\n",
    "  - Predictions on single/multiple inputs\n",
    "  - Manual forward/backward shape checks\n",
    "  - Numerical stability via `log(AL + 1e-8)`\n",
    "\n",
    "### Dataset: Sign Language Digits\n",
    "We use a dataset of **RGB images (64x64x3)** representing hand signs for digits **0–5**. Each image is labeled using **one-hot encoding**.  \n",
    "This task is ideal for evaluating the performance of a CNN in a controlled, small-scale image classification setting.\n",
    "\n",
    "### Goals of this Project\n",
    "- Understand CNN internals deeply\n",
    "- Build forward and backward pass logic manually\n",
    "- Learn how convolution and pooling interact\n",
    "- Visualize training performance with loss tracking\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This implementation is intended for educational purposes and performance/debugging clarity, not for production-scale deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from utils import load_signs_dataset, convert_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pads all images in the dataset X with zeros along height and width.\n",
    "\n",
    "    Arguments:\n",
    "    X -- numpy array of shape (m, n_H, n_W, n_C), batch of m images\n",
    "    pad -- int, padding size to apply on height and width dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- numpy array of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(\n",
    "        X,\n",
    "        pad_width=((0, 0), (pad, pad), (pad, pad), (0, 0)),\n",
    "        mode='constant',\n",
    "        constant_values=0\n",
    "    )\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step of Convolution \n",
    "\n",
    "Implementing a single step of convolution, in which we apply the filter to a single position of the input. This will be used to build a convolutional unit, which: \n",
    "\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Applies one filter (W, b) to a slice of input data (a_slice_prev).\n",
    "\n",
    "    Arguments:\n",
    "    a_slice_prev -- numpy array of shape (f, f, n_C_prev), slice from the input\n",
    "    W -- numpy array of shape (f, f, n_C_prev), filter weights\n",
    "    b -- numpy array of shape (1, 1, 1), bias term\n",
    "\n",
    "    Returns:\n",
    "    Z -- scalar, result of convolving W on a_slice_prev and adding bias\n",
    "    \"\"\"\n",
    "    s = a_slice_prev * W\n",
    "    Z = np.sum(s)\n",
    "    Z = Z + float(b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks - Forward Pass\n",
    "\n",
    "In the forward pass, we will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. We will then stack these outputs to get a 3D volume: \n",
    "\n",
    "    \n",
    "The formulas relating the output shape of the convolution to the input shape are:\n",
    "    \n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of a convolutional layer.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev), activations from previous layer\n",
    "    W -- numpy array of shape (f, f, n_C_prev, n_C), filters\n",
    "    b -- numpy array of shape (1, 1, 1, n_C), biases\n",
    "    hparameters -- dictionary with keys 'stride' and 'pad'\n",
    "\n",
    "    Returns:\n",
    "    Z -- numpy array of shape (m, n_H, n_W, n_C), output of the convolution\n",
    "    cache -- tuple of values needed for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, _, _, n_C) = W.shape\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "\n",
    "    # output dimensions\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "\n",
    "    # output Z\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    # Pad the input\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    # convolution\n",
    "    for i in range(m):  # loop over batch\n",
    "        a_prev_pad = A_prev_pad[i]     # (n_H_prev, n_W_prev, n_C_prev)\n",
    "        for h in range(n_H):  # loop over height\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):  # loop over width\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):  # loop over channels/filters\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    weights = W[:, :, :, c]     \n",
    "                    bias = b[:, :, :, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, bias)\n",
    "\n",
    "    # values for backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer \n",
    "\n",
    "The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: \n",
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. This specifies the height and width of the $f \\times f$ window we would compute a *max* or *average* over. \n",
    "\n",
    "As there's no padding, the formulas binding the output shape of the pooling to the input shape is:\n",
    "\n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f}{stride} \\Bigr\\rfloor +1$$\n",
    "\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f}{stride} \\Bigr\\rfloor +1$$\n",
    "\n",
    "$$n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev), input data\n",
    "    hparameters -- dictionary containing \"f\" (filter size) and \"stride\"\n",
    "    mode -- string, \"max\" or \"average\", specifies the pooling mode\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the pooling layer, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- tuple of (A_prev, hparameters), used for backprop\n",
    "    \"\"\"\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    # Output dimensions\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    # Pooling operation\n",
    "    for i in range(m):  # loop over batch\n",
    "        for h in range(n_H):  # loop over height\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):  # loop over width\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):  # loop over channels\n",
    "                    a_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice)\n",
    "\n",
    "    # Store cache for backward pass\n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(Z):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the ReLU activation.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation output, same shape as Z\n",
    "    cache -- Z, stored for computing backward pass\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z  # Needed for relu_backward\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a dense (fully connected) layer.\n",
    "\n",
    "    Computes:\n",
    "        Z = A_prev · W + b\n",
    "\n",
    "    Shapes:\n",
    "    - A_prev: (m, n_in)         # Input from previous layer\n",
    "    - W:      (n_in, n_out)     # Weights of current layer\n",
    "    - b:      (1, n_out)        # Biases, broadcasted to (m, n_out)\n",
    "    - Z:      (m, n_out)        # Linear output\n",
    "\n",
    "    Returns:\n",
    "    - Z: Linear output, shape (m, n_out)\n",
    "    - cache: tuple (A_prev, W, b), for use in backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(A_prev, W) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in Convolutional Neural Networks: The Core Idea\n",
    "At its heart, backpropagation is the application of the chain rule from calculus. When we train a neural network, we want to minimize a cost (or loss) function. To do this, we need to know how much each parameter (weights and biases) contributes to that cost. This \"contribution\" is measured by the gradient of the cost with respect to that parameter.\n",
    "\n",
    "The chain rule allows us to compute these gradients by working backward from the output of the network to the input, multiplying the gradients of each layer's output with respect to its input and parameters.\n",
    "\n",
    "-For a convolutional layer, the forward pass involves:\n",
    "\n",
    "- Taking a_slice (a patch from the input activation A_prev).\n",
    "\n",
    "- Performing an element-wise multiplication with a filter W_c.\n",
    "\n",
    "- Summing the results.\n",
    "\n",
    "- Adding a bias b_c.\n",
    "\n",
    "- This gives one element Z_hw in the output feature map Z.\n",
    "\n",
    "The backward pass reverses this. Given the gradient of the cost with respect to the output Z (denoted as dZ), we need to compute:\n",
    "\n",
    "- dA_prev: Gradient of the cost with respect to the input activation A_prev.\n",
    "\n",
    "- dW: Gradient of the cost with respect to the filters W.\n",
    "\n",
    "- db: Gradient of the cost with respect to the biases b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a Convolutional Layer (From Scratch)\n",
    "\n",
    "## Why Backprop?\n",
    "To update the parameters in a convolutional neural network, we must compute how the **loss `J`** changes with respect to:\n",
    "- the **input** (`dA_prev`)\n",
    "- the **filter weights** (`dW`)\n",
    "- the **bias** (`db`)\n",
    "\n",
    "This is done using **backpropagation** + **chain rule**.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass Recap (Single Step)\n",
    "\n",
    "Each element `Z[i, h, w, c]` is computed as:\n",
    "\n",
    "$$ Z_{i, h, w, c} = \\sum (a_{\\text{slice}} \\cdot W_c) + b_c $$\n",
    "\n",
    "Where:\n",
    "- `a_slice` is a slice from `A_prev` (shape = `(f, f, n_C_prev)`)\n",
    "- `W_c` is the filter for channel `c`\n",
    "- `b_c` is the bias\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Pass Goal\n",
    "\n",
    "We are given:\n",
    "- `dZ[i, h, w, c]`: gradient of cost with respect to `Z[i, h, w, c]`\n",
    "\n",
    "We need to compute:\n",
    "- `dA_prev`: Gradient wrt input activations\n",
    "- `dW`: Gradient wrt filters\n",
    "- `db`: Gradient wrt biases\n",
    "\n",
    "---\n",
    "\n",
    "## Chain Rule Intuition\n",
    "\n",
    "Let's define the full loss:\n",
    "\n",
    "$$\n",
    "J = \\text{Loss}(Z) \\quad \\Rightarrow \\quad \\frac{\\partial J}{\\partial A_{\\text{prev}}}\n",
    "$$\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A_{\\text{prev}}} = \\frac{\\partial J}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial A_{\\text{prev}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "\\quad ; \\quad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example (Single dZ)\n",
    "\n",
    "Imagine:\n",
    "- `a_slice_prev.shape = (f, f, n_C_prev)`\n",
    "- `W_c.shape = (f, f, n_C_prev)`\n",
    "- `dZ[i, h, w, c] = 1.5`  ← comes from future layers\n",
    "\n",
    "Then:\n",
    "- `dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, :] += W_c * dZ[i, h, w, c]`\n",
    "- `dW[:, :, :, c] += a_slice_prev * dZ[i, h, w, c]`\n",
    "- `db[:, :, :, c] += dZ[i, h, w, c]`\n",
    "\n",
    "> This is repeated over all (i, h, w, c) to accumulate all gradient contributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function.\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of cost w.r.t. output Z, shape (m, n_H, n_W, n_C)\n",
    "    cache -- Tuple of (A_prev, W, b, hparameters) from conv_forward\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient w.r.t. input A_prev, shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- Gradient w.r.t. weights W, shape (f, f, n_C_prev, n_C)\n",
    "    db -- Gradient w.r.t. biases b, shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack cache\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "\n",
    "    # Dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "\n",
    "    # Initialize gradients\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "\n",
    "    # Loop over all training examples\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    # Define the slice's corners\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Slice input & corresponding dZ\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    dz = dZ[i, h, w, c]\n",
    "\n",
    "                    # Compute gradients\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dz\n",
    "                    dW[:, :, :, c] += a_slice * dz\n",
    "                    db[:, :, :, c] += dz\n",
    "\n",
    "        # Unpad for this training example\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :] if pad != 0 else da_prev_pad\n",
    "\n",
    "    # check\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation in Pooling Layers: Masking and Distributing\n",
    "\n",
    "When performing backpropagation through a **pooling layer**, the way we handle the gradient depends on the **type of pooling** used:\n",
    "\n",
    "- **Max Pooling**: Only the maximum value in the pooling window affects the output — so only that position gets the gradient.\n",
    "- **Average Pooling**: All values in the window contribute equally to the output — so the gradient is **evenly distributed** across the window.\n",
    "\n",
    "We handle these cases with two helper functions:\n",
    "\n",
    "\n",
    "`create_mask_from_window(x)`  \n",
    "`distribute_value(dz, shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, marking the position of the maximum value.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): A window (e.g., a 2D or 3D slice of input activation)\n",
    "                        from which to create the mask.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A boolean mask of the same shape as x, where True indicates\n",
    "                    the position of the maximum value.\n",
    "    \"\"\"\n",
    "    max_value = np.max(x)\n",
    "    mask = (x == max_value)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"    \n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_W*n_H)\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it Together: Pooling Backward \n",
    "\n",
    "We now have everything you need to compute backward propagation on a pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Gradient of cost w.r.t. the output of the pooling layer, shape (m, n_H, n_W, n_C)\n",
    "    cache -- Tuple of (A_prev, hparameters), from the forward pass\n",
    "    mode -- Pooling mode: \"max\" or \"average\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of cost w.r.t. the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, hparameters = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "\n",
    "    # Loop over the training examples\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "\n",
    "        for h in range(n_H):             # loop over output height\n",
    "            for w in range(n_W):         # loop over output width\n",
    "                for c in range(n_C):     # loop over channels\n",
    "\n",
    "                    # Find corners of the slice\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Select the slice from the input\n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Create a mask of where the max value was during forward pass\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "\n",
    "                        # Backpropagate only to the max location\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "\n",
    "                    elif mode == \"average\":\n",
    "                        # Distribute the gradient equally to all positions\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        distributed = distribute_value(da, shape)\n",
    "\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distributed\n",
    "\n",
    "    # check\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Backward pass for ReLU activation.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activation\n",
    "    Z -- pre-activation value (output of conv or dense before ReLU)\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of features going into this dense layer = 𝑛_in\n",
    "- Number of units in the dense layer = 𝑛_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a dense (fully connected) layer using cache.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Gradient of cost w.r.t. Z (output of the layer), shape: (m, n_out)\n",
    "    cache -- Tuple of values (A_prev, W, b) from the forward pass\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient w.r.t. A_prev, shape: (m, n_in)\n",
    "    dW -- Gradient w.r.t. W, shape: (n_in, n_out)\n",
    "    db -- Gradient w.r.t. b, shape: (1, n_out)\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = np.dot(A_prev.T, dZ) / m\n",
    "    db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax + Cross-Entropy Backward Derivative\n",
    "\n",
    "The softmax function is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{Z_i}}{\\sum_{j=1}^{C} e^{Z_j}}\n",
    "$$\n",
    "\n",
    "The cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "J = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Combined together for a batch of \\( m \\) examples:\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{m} \\sum_{k=1}^{m} \\sum_{i=1}^{C} y_i^{(k)} \\log(\\hat{y}_i^{(k)})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "By applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z_i} = \\frac{\\partial J}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial Z_i}\n",
    "$$\n",
    "\n",
    "But due to the simplification when softmax is **combined** with cross-entropy, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "In vectorized form for all training examples:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z} = \\frac{1}{m} \\left( \\hat{Y} - Y \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ \\hat{Y} \\in \\mathbb{R}^{(m, C)} $ : softmax outputs  \n",
    "- $ Y \\in \\mathbb{R}^{(m, C)} $ : one-hot encoded true labels  \n",
    "- $ m $ : batch size  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_backward(preds, labels):\n",
    "    \"\"\"\n",
    "    Backward pass for softmax with cross-entropy loss.\n",
    "    Arguments:\n",
    "    preds -- softmax outputs (A3)   #  (m, n_classes)\n",
    "    labels -- one-hot encoded true labels (Y)    # (m, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "    dZ3 -- Gradient of the cost w.r.t. Z3\n",
    "    \"\"\"\n",
    "    m = labels.shape[0]\n",
    "    dZ3 = (preds - labels) / m\n",
    "    return dZ3  # (m, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # stability\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters, epoch=None):\n",
    "    \"\"\"\n",
    "    Implements the full forward pass:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input image batch, shape: (m, n_H, n_W, n_C)\n",
    "    parameters -- dictionary containing all weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    AL -- output probabilities (after dense + softmax)\n",
    "    caches -- list of caches for all layers (for backprop)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "  \n",
    "    # --------- Layer 1: CONV -> RELU -> POOL ----------\n",
    "    if epoch == 1:\n",
    "        print(\"=== Forward Pass ===\")\n",
    "        print(\"Layer 1: CONV -> RELU -> MAXPOOL\")\n",
    "        print(\"Input X shape:\", X.shape)\n",
    "\n",
    "    Z1, cache_Z1 = conv_forward(X, parameters[\"W1\"], parameters[\"b1\"], {'stride': 1, 'pad': 2})\n",
    "    if epoch == 1:\n",
    "        print(\"Z1 (after conv1) shape:\", Z1.shape)\n",
    "\n",
    "    A1, cache_A1 = relu_forward(Z1)\n",
    "    if epoch == 1:\n",
    "        print(\"A1 (after relu1) shape:\", A1.shape)\n",
    "\n",
    "    P1, cache_P1 = pool_forward(A1, {'f': 8, 'stride': 8}, mode=\"max\")\n",
    "    if epoch == 1:\n",
    "        print(\"P1 (after pool1) shape:\", P1.shape)\n",
    "        print('-' * 50)\n",
    "\n",
    "    caches += [cache_Z1, cache_A1, cache_P1]\n",
    "    \n",
    "    # --------- Layer 2: CONV -> RELU -> POOL ----------\n",
    "    if epoch == 1:\n",
    "        print(\"Layer 2: CONV -> RELU -> MAXPOOL\")\n",
    "\n",
    "    Z2, cache_Z2 = conv_forward(P1, parameters[\"W2\"], parameters[\"b2\"], {'stride': 1, 'pad': 2})\n",
    "    if epoch == 1:\n",
    "        print(\"Z2 (after conv2) shape:\", Z2.shape)\n",
    "\n",
    "    A2, cache_A2 = relu_forward(Z2)\n",
    "    if epoch == 1: \n",
    "        print(\"A2 (after relu2) shape:\", A2.shape)\n",
    "   \n",
    "    P2, cache_P2 = pool_forward(A2, {'f': 4, 'stride': 4}, mode=\"max\")\n",
    "    if epoch == 1:\n",
    "        print(\"P2 (after pool2) shape:\", P2.shape)\n",
    "\n",
    "    caches += [cache_Z2, cache_A2, cache_P2]\n",
    "    if epoch == 1:\n",
    "        print('-' * 50)\n",
    "\n",
    "    # --------- Flatten Layer ----------\n",
    "    if epoch == 1:\n",
    "        print(\"Flatten Layer\")\n",
    "    F = P2.reshape(P2.shape[0], -1)\n",
    "    if epoch == 1:\n",
    "        print(\"F (flattened) shape:\", F.shape)\n",
    "\n",
    "    cache_F = P2\n",
    "    caches.append(cache_F)\n",
    "\n",
    "    # --------- Fully Connected (Dense) Layer ----------\n",
    "    if epoch == 1:\n",
    "        print(\"Dense Layer + Softmax\")\n",
    "\n",
    "    Z3, cache_Z3 = dense_forward(F, parameters[\"W3\"], parameters[\"b3\"])\n",
    "    if epoch == 1:\n",
    "        print(\"Z3 (after dense) shape:\", Z3.shape)\n",
    "\n",
    "    A3 = softmax(Z3)\n",
    "    if epoch == 1:\n",
    "        print(\"A3 (after softmax) shape:\", A3.shape)\n",
    "\n",
    "    caches.append(cache_Z3)\n",
    "\n",
    "    return A3, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches, epoch=None):\n",
    "    \"\"\"\n",
    "    Implements the full backward pass:\n",
    "    DENSE -> FLATTEN -> MAXPOOL -> RELU -> CONV -> MAXPOOL -> RELU -> CONV\n",
    "\n",
    "    Arguments:\n",
    "    AL -- predictions from forward pass, shape (m, n_classes)\n",
    "    Y -- true labels (one-hot), shape (m, n_classes)\n",
    "    caches -- list of caches from forward pass\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary containing gradients for all weights, biases, and activations\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "\n",
    "    # Unpack caches\n",
    "    cache_Z1, cache_A1, cache_P1, \\\n",
    "    cache_Z2, cache_A2, cache_P2, \\\n",
    "    cache_F, cache_Z3 = caches\n",
    "\n",
    "    if epoch == 1:\n",
    "        print(\"=== Backward Pass ===\")\n",
    "        print(\"AL shape:\", AL.shape, \" | Y shape:\", Y.shape)\n",
    "\n",
    "    # Last layer: softmax + cross-entropy\n",
    "    dZ3 = softmax_cross_entropy_backward(AL, Y)\n",
    "    dA2_flat, dW3, db3 = dense_backward(dZ3, cache_Z3)\n",
    "    if epoch == 1:\n",
    "        print(\"-> Dense backward: dW3:\", dW3.shape, \", db3:\", db3.shape, \", dA2_flat:\", dA2_flat.shape)\n",
    "\n",
    "    grads[\"dW3\"] = dW3\n",
    "    grads[\"db3\"] = db3\n",
    "\n",
    "    # Reshape back to conv output shape\n",
    "    dP2 = dA2_flat.reshape(cache_F.shape)\n",
    "    if epoch == 1:\n",
    "        print(\"-> Reshape dA2_flat to dP2:\", dP2.shape)\n",
    "\n",
    "    # Backprop through pool2\n",
    "    dA2 = pool_backward(dP2, cache=(cache_A2, {'f': 4, 'stride': 4}), mode=\"max\")\n",
    "    if epoch == 1:\n",
    "        print(\"-> Pool2 backward: dA2:\", dA2.shape)\n",
    "\n",
    "    # Backprop through ReLU2\n",
    "    dZ2 = relu_backward(dA2, cache_A2)\n",
    "    if epoch == 1:\n",
    "        print(\"-> ReLU2 backward: dZ2:\", dZ2.shape)\n",
    "\n",
    "    # Backprop through conv2\n",
    "    dP1, dW2, db2 = conv_backward(dZ2, cache_Z2)\n",
    "    if epoch == 1:\n",
    "        print(\"-> Conv2 backward: dW2:\", dW2.shape, \", db2:\", db2.shape, \", dP1:\", dP1.shape)\n",
    "\n",
    "    grads[\"dW2\"] = dW2\n",
    "    grads[\"db2\"] = db2\n",
    "\n",
    "    # Backprop through pool1\n",
    "    dA1 = pool_backward(dP1, cache=(cache_A1, {'f': 8, 'stride': 8}), mode=\"max\")\n",
    "    if epoch == 1:\n",
    "        print(\"-> Pool1 backward: dA1:\", dA1.shape)\n",
    "\n",
    "    # Backprop through ReLU1\n",
    "    dZ1 = relu_backward(dA1, cache_A1)\n",
    "    if epoch == 1:\n",
    "        print(\"-> ReLU1 backward: dZ1:\", dZ1.shape)\n",
    "\n",
    "    # Backprop through conv1\n",
    "    dX, dW1, db1 = conv_backward(dZ1, cache_Z1)\n",
    "    if epoch == 1:\n",
    "        print(\"-> Conv1 backward: dW1:\", dW1.shape, \", db1:\", db1.shape, \", dX:\", dX.shape)\n",
    "\n",
    "    grads[\"dW1\"] = dW1\n",
    "    grads[\"db1\"] = db1\n",
    "    grads[\"dX\"] = dX\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weights and biases for the custom CNN:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized W and b for each layer\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\": np.random.randn(5, 5, 3, 8) * 0.1,\n",
    "        \"b1\": np.zeros((1, 1, 1, 8)),\n",
    "        \"W2\": np.random.randn(5, 5, 8, 16) * 0.1,\n",
    "        \"b2\": np.zeros((1, 1, 1, 16)),\n",
    "        \"W3\": np.random.randn(64, 6) * 0.1,\n",
    "        \"b3\": np.zeros((1, 6))\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, parameters, learning_rate=0.01, num_epochs=20, print_every=1):\n",
    "    \"\"\"\n",
    "    Trains the custom CNN model using forward and backward passes.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training images, shape (m_train, 64, 64, 3)\n",
    "    Y_train -- one-hot labels, shape (m_train, 6)\n",
    "    parameters -- initialized model parameters\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_epochs -- number of training epochs\n",
    "    print_every -- prints cost and accuracy every n epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned weights and biases after training\n",
    "    losses -- list of loss values per epoch\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        AL, caches = model_forward(X_train, parameters, epoch+1)\n",
    "\n",
    "        # Compute loss (cross-entropy)\n",
    "        loss = -np.mean(np.sum(Y_train * np.log(AL + 1e-8), axis=1))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backward pass\n",
    "        grads = model_backward(AL, Y_train, caches, epoch+1)\n",
    "\n",
    "        # Update parameters\n",
    "        for l in [1, 2, 3]:\n",
    "            parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "            parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "        # Print loss every print_every epochs\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    return parameters, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained model.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (m, 64, 64, 3)\n",
    "    parameters -- trained weights and biases\n",
    "\n",
    "    Returns:\n",
    "    predictions -- numpy array of shape (m,), predicted class indices\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    AL, _ = model_forward(X, parameters)  # AL: shape (m, 6), probabilities\n",
    "\n",
    "    # Take argmax across classes (axis=1)\n",
    "    predictions = np.argmax(AL, axis=1)  # shape (m,)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(X, Y_true, parameters):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model on given data.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (m, 64, 64, 3)\n",
    "    Y_true -- true labels, shape (1, m) or (m,)\n",
    "    parameters -- trained model parameters\n",
    "\n",
    "    Returns:\n",
    "    acc -- accuracy (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    preds = predict(X, parameters)\n",
    "\n",
    "    if Y_true.ndim == 2 and Y_true.shape[0] == 1:\n",
    "        Y_true = Y_true.flatten()  # convert (1, m) -> (m,)\n",
    "    elif Y_true.ndim == 2 and Y_true.shape[1] == 1:\n",
    "        Y_true = Y_true[:, 0]\n",
    "\n",
    "    acc = np.mean(preds == Y_true)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1, 1080)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (1, 120)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train_orig.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test_orig.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train_orig.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train_orig.shape))\n",
    "print (\"X_test shape: \" + str(X_test_orig.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 5\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train_small = 50\n",
    "X_train_small = X_train[:m_train_small]    \n",
    "Y_train_small = Y_train[:m_train_small]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward Pass ===\n",
      "Layer 1: CONV -> RELU -> MAXPOOL\n",
      "Input X shape: (50, 64, 64, 3)\n",
      "Z1 (after conv1) shape: (50, 64, 64, 8)\n",
      "A1 (after relu1) shape: (50, 64, 64, 8)\n",
      "P1 (after pool1) shape: (50, 8, 8, 8)\n",
      "--------------------------------------------------\n",
      "Layer 2: CONV -> RELU -> MAXPOOL\n",
      "Z2 (after conv2) shape: (50, 8, 8, 16)\n",
      "A2 (after relu2) shape: (50, 8, 8, 16)\n",
      "P2 (after pool2) shape: (50, 2, 2, 16)\n",
      "--------------------------------------------------\n",
      "Flatten Layer\n",
      "F (flattened) shape: (50, 64)\n",
      "Dense Layer + Softmax\n",
      "Z3 (after dense) shape: (50, 6)\n",
      "A3 (after softmax) shape: (50, 6)\n",
      "=== Backward Pass ===\n",
      "AL shape: (50, 6)  | Y shape: (50, 6)\n",
      "-> Dense backward: dW3: (64, 6) , db3: (1, 6) , dA2_flat: (50, 64)\n",
      "-> Reshape dA2_flat to dP2: (50, 2, 2, 16)\n",
      "-> Pool2 backward: dA2: (50, 8, 8, 16)\n",
      "-> ReLU2 backward: dZ2: (50, 8, 8, 16)\n",
      "-> Conv2 backward: dW2: (5, 5, 8, 16) , db2: (1, 1, 1, 16) , dP1: (50, 8, 8, 8)\n",
      "-> Pool1 backward: dA1: (50, 64, 64, 8)\n",
      "-> ReLU1 backward: dZ1: (50, 64, 64, 8)\n",
      "-> Conv1 backward: dW1: (5, 5, 3, 8) , db1: (1, 1, 1, 8) , dX: (50, 64, 64, 3)\n",
      "Epoch 1/10 - Loss: 1.9711\n",
      "Epoch 2/10 - Loss: 1.9316\n",
      "Epoch 3/10 - Loss: 1.9028\n",
      "Epoch 4/10 - Loss: 1.8815\n",
      "Epoch 5/10 - Loss: 1.8653\n",
      "Epoch 6/10 - Loss: 1.8527\n",
      "Epoch 7/10 - Loss: 1.8424\n",
      "Epoch 8/10 - Loss: 1.8336\n",
      "Epoch 9/10 - Loss: 1.8264\n",
      "Epoch 10/10 - Loss: 1.8200\n"
     ]
    }
   ],
   "source": [
    "trained_params, losses = train_model(X_train_small, Y_train_small, parameters, num_epochs=10, learning_rate=0.01, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, marker='o', color='blue')\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
